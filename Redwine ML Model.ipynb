{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5541d8f3",
   "metadata": {},
   "source": [
    "# Module 2: Assignment - Real-world Application of Supervised Learning\n",
    "\n",
    "Apply supervised learning techniques to a real-world dataset to solve a prediction problem. Use at least two different supervised learning algorithms to train models and perform a comparative analysis of their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c928abb",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Predict the quality of wine based on its chemical properties using supervised learning techniques. We will use the Wine Quality dataset, which includes 11 features such as acidity, residual sugar, and alcohol content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0ed69",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Using the \"Wine Quality\" dataset from the UCI Machine Learning Repository which involves predicting the quality of the wine based on various chemical properties.\n",
    "\n",
    "Starting with data preprocessing\n",
    "- Loading the data and check for missing values\n",
    "- Transformations (Enconding, Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "# Check for first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2133c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the target variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'quality' is the target variable\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ab28f",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Using two algorithms\n",
    "1. RandomForestRegressor\n",
    "2. GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31de213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest Regressor:  {'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 20, 30, None]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_rf = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Log best parameters and model\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(\"Best parameters for RandomForestRegressor: \", grid_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed49bab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting Regressor:  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "grid_gb = GridSearchCV(gb, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_gb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Log best parameters and model\n",
    "best_gb = grid_gb.best_estimator_\n",
    "print(\"Best parameters for GradientBoostingRegressor: \", grid_gb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab7f566",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "182283d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor:\n",
      "Mean Squared Error:  0.29322270970257763\n",
      "Mean Absolute Error:  0.4150765587773916\n",
      "R^2 Score:  0.5513082162981522\n",
      "\n",
      "Gradient Boosting Regressor:\n",
      "Mean Squared Error:  0.3473442409283599\n",
      "Mean Absolute Error:  0.44086744142218065\n",
      "R^2 Score:  0.46849100747076233\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Random Forest Regressor\n",
    "y_pred_rf = best_rf.predict(X_test_scaled)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Regressor:\")\n",
    "print(\"Mean Squared Error: \", mse_rf)\n",
    "print(\"Mean Absolute Error: \", mae_rf)\n",
    "print(\"R^2 Score: \", r2_rf)\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "y_pred_gb = best_gb.predict(X_test_scaled)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(\"\\nGradient Boosting Regressor:\")\n",
    "print(\"Mean Squared Error: \", mse_gb)\n",
    "print(\"Mean Absolute Error: \", mae_gb)\n",
    "print(\"R^2 Score: \", r2_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0aa86",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "In this analysis, I compare the performance of two supervised learning algorithms, Random Forest Regressor and Gradient Boosting Regressor, in predicting the quality of wine based on various chemical properties. The performance metrics used for this comparison are Mean Squared Error (MSE), Mean Absolute Error (MAE), and R^2 Score.\n",
    "\n",
    "### Performance Metrics\n",
    "**Random Forest Regressor:**\n",
    "- **Mean Squared Error (MSE):** 0.2932\n",
    "- **Mean Absolute Error (MAE):** 0.4151\n",
    "- **R^2 Score:** 0.5513\n",
    "\n",
    "**Gradient Boosting Regressor:**\n",
    "- **Mean Squared Error (MSE):** 0.3473\n",
    "- **Mean Absolute Error (MAE):** 0.4409\n",
    "- **R^2 Score:** 0.4685\n",
    "\n",
    "### Interpretation of Results\n",
    "- **Mean Squared Error (MSE):**\n",
    "  - The MSE for the Random Forest Regressor is lower (0.2932) compared to the Gradient Boosting Regressor (0.3473). This indicates that the Random Forest Regressor has a better fit to the data, as it has a lower average squared difference between the predicted and actual values.\n",
    "- **Mean Absolute Error (MAE):**\n",
    "  - The MAE for the Random Forest Regressor is also lower (0.4151) compared to the Gradient Boosting Regressor (0.4409). This suggests that the Random Forest Regressor's predictions are, on average, closer to the actual values than those of the Gradient Boosting Regressor.\n",
    "- **R^2 Score:**\n",
    "  - The R^2 Score for the Random Forest Regressor is higher (0.5513) compared to the Gradient Boosting Regressor (0.4685). The R^2 Score represents the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher R^2 Score indicates a better fit of the model to the data.\n",
    "\n",
    "### Strengths and Weaknesses\n",
    "**Random Forest Regressor:**\n",
    "- **Strengths:**\n",
    "  - **Performance:** Exhibits better performance with lower MSE and MAE, and a higher R^2 Score.\n",
    "  - **Robustness:** Less sensitive to overfitting due to averaging multiple decision trees.\n",
    "  - **Feature Importance:** Provides insights into feature importance, which can be valuable for understanding the model.\n",
    "- **Weaknesses:**\n",
    "  - **Computationally Intensive:** Can be computationally expensive, especially with a large number of trees and features.\n",
    "  - **Interpretability:** Individual trees are interpretable, but the overall model can be complex to interpret.\n",
    "\n",
    "**Gradient Boosting Regressor:**\n",
    "- **Strengths:**\n",
    "  - **Accuracy:** Can provide high predictive accuracy through boosting, by sequentially correcting errors of the weak learners.\n",
    "  - **Flexibility:** Capable of handling various loss functions and providing robust performance.\n",
    "- **Weaknesses:**\n",
    "  - **Overfitting:** More prone to overfitting, especially with a high number of boosting stages.\n",
    "  - **Training Time:** Generally requires more training time due to the sequential nature of boosting.\n",
    "\n",
    "### Conclusion\n",
    "Based on the evaluation metrics, the Random Forest Regressor outperforms the Gradient Boosting Regressor in this specific task of predicting wine quality. The Random Forest Regressor shows lower MSE and MAE, and a higher R^2 Score, indicating better overall performance. However, both models have their respective strengths and weaknesses.\n",
    "\n",
    "For tasks where interpretability and computational efficiency are critical, the Random Forest Regressor may be preferable. On the other hand, for scenarios requiring high predictive accuracy and flexibility, despite the potential for overfitting, the Gradient Boosting Regressor can be a strong candidate.\n",
    "\n",
    "Future work could involve further hyperparameter tuning, feature engineering, and exploring other advanced algorithms to potentially enhance predictive performance. This analysis demonstrates the importance of evaluating multiple models to select the best one for a given predictive task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
